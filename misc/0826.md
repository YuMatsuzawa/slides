## または私は如何にして心配するのをやめてReducerにJoin処理を強いるようになったか

RnD 松澤

---

## 今日はHadoopのおはなし

* MapReduceおさらい
* Hadoopおさらい
* MapReduceに向かない処理を無理矢理MapReduceする
* （物によっては）もう少し賢くできる
* Jubatusちょっと紹介

---

## MapReduceおさらい

* 分散並列処理が可能な問題に対するアプローチ。
 * 簡単な例）WordCount

^

Map関数とReduce関数はもはやあまり関係ない。

>>>

![MapReduce](http://www.pinaldave.com/bimg/mapreduce.jpg)

>>>

## Mapper

与えられたデータを`Key`と`Value`のペアに分けて放出する

## Reducer

同一の`Key`を持つデータの`Value`のリストを指定方法で集計する

>>>

### Shuffle

Mapperの出力を同一`Key`ごとに分け、  
ソートしてReducerに渡す処理

### Partitioner

同一`Key`でまとめられたデータを、  
複数いるReducerに割り振る

### Combiner

中間集計を行う。Reducerとほぼ同一の構造だが、  
入力と出力の形式がMapperとReducerの間を橋渡しできる必要あり

>>>

## 簡単な例：WordCount

* 分散・並列処理が可能な問題の筆頭？
* 象本とか『Hadoop徹底入門』とか
* 以下コードはJava、それもhadoop-0.20系準拠
 * 基本変わってないはず。新し目のApacheHadoopとかCDHでも動く

>>>

## Mapper

```java
public static class Map
extends Mapper<LongWritable, Text, Text, IntWritable> {
  private final static IntWritable one = new IntWritable(1);
  private Text word = new Text();

  @Override
  protected void map(LongWritable key, Text value, Context context)
  throws IOException, InterruptedException {
    String line = value.toString();
    //スペースでsplit
    StringTokenizer tokenizer = new StringTokenizer(line);
    while (tokenizer.hasMoreTokens()) {
      word.set(tokenizer.nextToken());
      //単語を1個ずつ放出
      context.write(word, one);
    }
  }
}
```

>>>

## Reducer

```java
public static class Reduce
extends Reducer<Text, IntWritable, Text, IntWritable> {
  @Override
  protected void reduce(Text key, Iterable<IntWritable> values, Context context)
  throws IOException, InterruptedException {
    int sum = 0;
    for (IntWritable value : values) {
      int n = value.get(); //数字を読んで
      sum += n;            //足す
    }
    //keyは単語、sumはvalueの合計値
    context.write(key, new IntWritable(sum));
  }
}
```

>>>

## Combiner

(使うなら)
```java
public static class Combine　extends Reduce {};
```

* CombinerはMapper出力しか受け取らない
* ReducerはMapper__あるいは__Combiner出力を受け取る
 * Combinerが呼ばれるか否かはMap処理の重さによる
 * 必ず呼ばれるとは限らない
* 上記2点を踏まえて、Reducerを作る必要あり
 * 今回のReducerは大丈夫
   * 初期値の1でも、中間集計値でも合計できる

>>>

## 起こること

### Mapper入力（元データ）

```no-highlight
I as a writer think that that that that that writer used
was wrong in that that that was in the wrong position.
```

* `TextInputFormat`の場合:
 * 行頭indexが`Key`
 * 行内容が`Value`
* `SequenceFileInputFormat`  
(Hadoop用のシリアル形式)の場合:
 * 任意の`Key-Value`構造を持たせられる

>>>

### Mapper出力
```no-highlight
<"I",1>,<"as",1>,<"a",1>,<"writer",1>,<"think",1>,
<"that",1>,<"that",1>,<"that",1>,<"that",1>,<"that",1>,
<"writer",1>,<"used",1>,<"was",1>,<"wrong",1>,<"in",1>,
<"that",1>,<"that",1>,<"that",1>,<"was",1>,<"in",1>,
<"the",1>,<"wrong",1>,<"position.",1>
```
* `Value`はすべて`1`。コード参照

>>>

### Reducer入力
```no-highlight
<"a",[1]>,
<"as",[1]>,
<"in",[1,1]>,
<"I",[1]>,
<"position.",[1]>,
<"that",[1,1,1,1,1,1,1,1]>,
<"the",[1]>,
<"think",[1]>,
<"used",[1]>,
<"was",[1,1]>,
<"writer",[1,1]>,
<"wrong",[1,1]>,
```

* `Key`でソートの上、`Value`がリスト化されている

^

この処理は実装上はMapperクラスの中で行われているらしい。
したがってMapperの出力はすでにこの形。これがReducerもしくはCombinerに入る。

>>>

### Reducer出力
```no-highlight
<"a",1>,
<"as",1>,
<"in",2>,
<"I",1>,
<"position.",1>,
<"that",8>,
<"the",1>,
<"think",1>,
<"used",1>,
<"was",2>,
<"writer",2>,
<"wrong",2>,
```

* めでたし

---

## Hadoopおさらい

* MapReduceフレームワークで分散プログラミングできる
* そのためのソフトウェアフレームワークがHadoop
* 基本的にはHDFSとMapReduceからなる
  * 最近のバージョンはもっと他にもサービスたくさん

>>>

![Architecture](http://opensource.com/sites/default/files/resize/images/life-uploads/hadoop-HighLevel_hadoop_architecture-640x460.png)

>>>

## HDFS

* Hadoop Distributed File System
  * 分散ファイルシステム
* Master（Name Node）とSlaves（Data Node）
  * NNはメタデータを管理
  * DNは実際のデータブロックを保持

>>>

![HDFS](http://www.cloudera.co.jp/wp-content/uploads/2014/04/HDFS_1.png)

1. データはブロック単位に分割され、DN間で分担保持
2. すべてのブロックは複製され、複数のDNに同時存在
3. Rack-aware: ネットワーク構造を自動解析し、  
レプリカを別ラックに配置する
4. BalancerがDNのディスク使用率を概ね均等に保つ

>>>

![SPOF](http://blog.cloudera.com/wp-content/uploads/2012/03/HANNdiagram-2.png)

>>>

* NNは単一障害点
  * よってHDFSはCPシステム
  * 以前のバージョンではNNの障害時は手動復旧が必須
  * 最近は一応High Availability設定がある
    * Active NN -> Standby NNへのFailover

>>>

## MapReduce on Hadoop

![MR](https://farm3.static.flickr.com/2344/3529959486_8f36fb28c5_o.png)

>>>

* 基本的な思想：
  * データはすでに分けられている(HDFS)  
→自分(DN)が持ってる部分のMapは自分でやる  
→Shuffleした後のReduceは均等に再分配
  * Map処理がFailした/DNがクラッシュしたら  
→レプリカを持ってる別のDN上でリトライ
  * ReduceがFailしたら  
→別ノード上に再分配してリトライ

>>>

* 以前は：
  * JobTracker（Master）
  * TaskTracker（Slave）で構成
* 最近のバージョン（YARN）は：
  * ResourceManager（全体監視・最適化）
  * ApplicationMaster（Job単位の管理）
  * NodeMaster（ノード単位の管理）

^

YARNによる賢いチューニングは複雑

>>>

## 何が結局すごいのか

* なかなかフォールトトレラントだ
  * データは(少なくとも2ノードあれば)最初から冗長化
    * Raidいらず
  * 子タスクのFailは自動的にリトライ
* コモディティPCでも簡単に導入・スケール
  * 学生が卒業してPCが余った？→クラスタに入れよう
  * 250GBのHDDが余ってる？→DNに入れよう
  * といってもメモリ8Gはあった方がいい。。
    * 最近のバージョンはサービスが多い  
    →デーモン群がそこそこメモリを食う
  * 調子に乗るとどんどんディスクが足りなくなる

>>>

## 使ってみたい？

1. AWSでやる
  * EMRでやる
    * 若干高いらしい
  * 複数EC2クラスタにHadoop組む
    * 若干安いらしい
2. 手で組む
  * 個人的にはこれしかやったことない
    * 多少は相談乗れます
    * CDH5を使えば楽です

---

## MapReduceに向かない処理を無理矢理MapReduceする

* そもそもMapReduceに向いているのは：
> 分散並列処理が可能な問題

>>>

## WordCountは

* 向いてた。小分けにして数えて合計すればいいもん
* これに限らず、「分割」「合計、集計」できる問題は  
だいたいMapReduceできそう
  * 例）同一モデルでパラメータを変えていくシミュレーション
  * 例）モンテカルロ法による円周率推定
  * 単純な分散コンピューティングクラスタとしても使える

>>>

## 小分けにできない問題

* 別のデータとの比較が必要な問題
  * 同一ファイルのn行目の値とm行目の値で比較
    * データは分割されている  
    →同一ノードにないかも。  
    →Mapper内で記憶しようか？
  * 組み合わせ総当たり
    * 記憶するにしても入力は同一ノード内からしか来ない
    * 組み合わせに漏れ

>>>

## 小分けにできるようにする

* Join
  * データベースの特定規則に基づいた結合
* CrossJoin（**禁忌**）
  * 交差結合（直積）を取る
  * つまり2つのデータソース間で可能な組み合わせを網羅した結合データベースを作る
    * 1つのデータソースでやってもいい
  * 激重
    * 100行x100行＝10,000行  
    （順序不定なら半分で約5,000行）
    * 1行平均1kBなら  
    →元データは100kB→出力10MB  
    →1000行（1MB）なら出力1GB

>>>

## なぜそんな重い操作を

* 1回やってしまえばMapReduceできる
  * 組合せを取るというHadoopに向かない行為だけを抽出
  * 全組み合わせデータベースがあれば：
    * Mapperにはペア単位で入力される
    * 両者の情報が常にMapperに渡る
  * MapReduce自体はやっぱり使いたい
    * 単一PCじゃやってられない
* 具体例：
  * 二部グラフを用いたデータ分類（前回発表）
    * 類似度ネットワークの構成に必要


>>>

## やり方

```java
void map(LongWritable key, Text value, Context context) {
  context.write(new IntWritable(1), value);
}

// Keyは`1`しかないのですべてのレコードが単一のReducerに集まる
void reduce(IntWritable key, Iterable<Text> values, Context context) {
  ArrayList<Text> valList = new ArrayList<Text>();
  while(values.hasMoreTokens) valList.add(values.nextToken);
  for(int cur = 0; cur < valList.size(); cur++) {
    for(int innerCur = 0; innerCur < valList.size(); innerCur++) {
      if(innerCur < cur)
        context.write(valList.get(cur), valList.get(innerCur));
      else
        break;
    }
  }
}
```

>>>

## 孤軍奮闘するReducer

* 1ノード（1Reducer）に全レコードが集まればCrossJoin可能
* といっても巨大ファイルはキツい
  * 1GBファイルなら前出の例だと出力は1PB
    * お手製クラスタではしんどい
  * 出力が~1TB程度に収まるよう元データは絞る
* 今考えると多分この問題はもっと小単位に分割できる気がする。。

>>>

## ジョブチェーン

* MapReduceのフレームワークは1ジョブでできることが限定的
  * MapperやReducerの入出力の形式が制約
  * 全体としての作業内容を複数ジョブに分け、チェーンさせる
    * `waitForCompletion`などのAPIが用意されている
  * 入出力データはいずれもHDFSに格納
    * 特にデフォルトの出力形式はそのまま別ジョブに入力可能
* JoinしてからMapReduceにかけるのもこの一種

---

## （物によっては）もう少し賢くできる

* `DistributedCache`という機能がある
  * 指定したデータの複製を全ノードに配布できる
  * 比較対象となるデータが比較的小さければ推奨手段
    * 大きいとメモリと帯域がやばいのでキツい
* 大規模データ(~GB,~TB)を、小規模データ(~MB)と比較・照合
  * 例）Tweetログから、特定ユーザに関連したもの、  
  特定Retweetに関連したユーザ、を抽出

>>>

* `DistributedCache`に入れるべきデータもMapReduceジョブで生成可能
* ワークフロー：
  1. 元データをHDFSにアップロード、必要ならCacheデータも
  2. 適切にフィルタリング・クレンジング（MapReduce）
  3. 必要であれば元データからさらにCacheデータを抽出・作成（MapReduce）
  4. Cacheを読みながらメインジョブチェーンを実行（MapReduce）
  5. 出力もHDFS上に吐き出し、必要ならダウンロード

>>>

## 注意点

* HDFSの想定は__Write Once, Read Many Times__
  * 書き換えを行わず、一度書き込んだら読むだけ、という状況において最高スループット
  * いわばimmutableなデータモデル
* ジョブチェーンの中間データもどんどんHDFSに吐かれる
  * HDFSの容量はどんどん足りなくなる
  * でもノード簡単に追加できるんだからいいよね！
* 自家でやるなら定期的なクリーニングとBalancer稼働は重要

---

## Jubatusちょっと紹介

* HadoopはHDFSに永続化されたデータを高速にバッチ処理できた
  * リアルタイム処理には適さない
    * とか言ってたらfacebookがHBase使いだした(2010)
    * このへんはワークロードの性質に依存
  * MapReduceが難しい
  * 機械学習アルゴとあまり適合しない

>>>

## 分散リアルタイム分析

![Jubatus](http://jubat.us/ja/_images/blockdiag-506457f227e8e9c8c70df500b639ba903a563948.png)

>>>

* AppサーバはJubatusProxyと同居
  * JubatusProxyはAppサーバからデータを受け取って分析クラスタに適切に分配
  * Appサーバが分析結果をProxyに要求すると、Proxyはクラスタからそれを取得
* 分析クラスタでは、学習モデルを__Mix処理__によって定期的に更新・共有
  * 任意のタイミングですべてのサーバが最新のモデルに基づく分析を行っているとは限らない
  * その代わりリアルタイム学習により常時モデルは更新され続ける

>>>

* 一般的な機械学習アルゴをだいたいサポート
  * 分類器、回帰、推薦、異常検知、etc.
* フローデータを扱えるのは強みだが、非同期対応が疑問
  * チュートリアルコードだと1入力1応答のRPC
* ストリームデータソースの扱い
  * Kafkaとの連携によって可能みたい？
  * まだまだ要調査
